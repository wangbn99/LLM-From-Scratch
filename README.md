# LLM-From-Scratch

This project implements a Generative Pre-trained Transformer (GPT) model from scratch. The most of the comments on the codes and unit tests were generated by Gemini.

## Project Structure

```
LLM-From-Scratch/
├── .gitignore                # Files and directories to ignore in version control
├── gpt_model.pt              # Pre-trained model checkpoint
├── .vscode/                  # VSCode configuration files
├── data/                     # Dataset directory
│   ├── train/                # Training data
│   ├── val/                  # Validation and test data
├── model/                    # Core model implementation
│   ├── __init__.py
│   ├── configuration.py      # Model configuration
│   ├── data_loader.py        # Batch iterator for training
│   ├── dataloader.py         # Dataset and DataLoader implementation (not use)
│   ├── modeling.py           # GPT model and components
│   ├── trainer.py            # Training loop and utilities
├── scripts/                  # Utility scripts
│   ├── download.py           # Dataset downloader
│   ├── generate.py           # Text generation script
│   ├── preprocess.py         # Data preprocessing script
│   ├── train.py              # Model training script
├── tests/                    # Unit tests
│   ├── test_data_loader.py   # Tests for data loader
│   ├── test_modeling.py      # Tests for model components
│   ├── test_trainer.py       # Tests for training logic
```

## Features

- **Model Implementation**: Implements GPT architecture with multi-head attention, feed-forward layers, and layer normalization.
- **Data Handling**: Preprocesses datasets and provides efficient batch iterators.
- **Training**: Includes a trainer class for model training with evaluation and checkpointing.
- **Text Generation**: Generates text using the trained model with options for temperature, top-k sampling, and early stopping.
- **Testing**: Comprehensive unit tests for all major components.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo/LLM-From-Scratch.git
   cd LLM-From-Scratch
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Ensure you have the required datasets. Use the `scripts/download.py` script to download them:
   ```bash
   python scripts/download.py
   ```

## Usage

### Preprocess Data
Preprocess the training and validation datasets. The parameters for this script are optional.
```bash
python scripts/preprocess.py --train_dir data/train --val_dir data/val
```

### Train the Model
Train the GPT model. The parameters for this script are optional.
```bash
python scripts/train.py
```

### Generate Text
Generate text using the trained model. The parameters for this script are optional.
```bash
python scripts/generate.py --model_path gpt_model.pt --input_text "Once upon a time" --max_new_tokens 50
```

### Run Tests
Run the unit tests to verify the implementation:
```bash
pytest tests/
```

## Configuration

The model configuration is defined in [`model/configuration.py`](model/configuration.py). Key parameters include:
- `vocab_size`: Size of the vocabulary.
- `context_length`: Maximum sequence length.
- `n_embd`: Embedding dimension.
- `n_heads`: Number of attention heads.
- `n_layers`: Number of transformer blocks.
- `dropout`: Dropout rate.

## File Descriptions

### Core Components
- [`model/modeling.py`](model/modeling.py): Implements the GPT model, including multi-head attention, feed-forward layers, and transformer blocks.
- [`model/trainer.py`](model/trainer.py): Handles training, evaluation, and checkpointing.
- [`model/data_loader.py`](model/data_loader.py): Provides a batch iterator for training data.

### Scripts
- [`scripts/download.py`](scripts/download.py): Downloads datasets from specified URLs.
- [`scripts/preprocess.py`](scripts/preprocess.py): Preprocesses datasets and saves them in HDF5 format.
- [`scripts/train.py`](scripts/train.py): Trains the GPT model.
- [`scripts/generate.py`](scripts/generate.py): Generates text using the trained model.

### Tests
- [`tests/test_modeling.py`](tests/test_modeling.py): Unit tests for model components.
- [`tests/test_data_loader.py`](tests/test_data_loader.py): Unit tests for data loading.
- [`tests/test_trainer.py`](tests/test_trainer.py): Unit tests for training logic.

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## Acknowledgments

- Inspired by the GPT architecture described in the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper.
- Uses datasets from [The Pile](https://pile.eleuther.ai/).

